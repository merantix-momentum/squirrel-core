{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2d7553",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/merantix/mxlabs-datasets/blob/main/examples/Squirrel_Tutorial_Create_Squirrel_Store.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50498336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keyring keyrings.google-artifactregistry-auth\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mxlabs-squirrel squirrel-datasets dask --extra-index=https://europe-west1-python.pkg.dev/mx-labs-devops/labs-pypi-registry/simple/ --ignore-requires-python --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba755167",
   "metadata": {},
   "source": [
    "If you have not already, refer to the documentation page for `Store` to cover the basics first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca45c2c",
   "metadata": {},
   "source": [
    "## Reading and writing to the store using Spark\n",
    "\n",
    "Squirrel makes it a breeze to scale out any data workload.\n",
    "To illustrate this using Spark, we:\n",
    "\n",
    "1. Create a data source\n",
    "\n",
    "2. Initialize a Driver that can read from the data source\n",
    "\n",
    "3. Construct an RDD in Spark from the data loaded using the Driver\n",
    "\n",
    "4. Create a DataFrame from the RDD and write data into shards using SquirrelStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96805124",
   "metadata": {},
   "source": [
    "Let's first create some dummy data and save it into a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04070b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "N_SAMPLES = 1_000\n",
    "\n",
    "\n",
    "def create_sample():\n",
    "    return {\n",
    "        \"name\": np.random.choice([\"John\", \"Jane\"]),\n",
    "        \"identifier\": int(np.random.choice([1, 2])),\n",
    "        \"age\": int(np.random.choice([20, 30])),\n",
    "    }\n",
    "\n",
    "\n",
    "samples = [create_sample() for _ in range(N_SAMPLES)]\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "csv_path = f\"{tmpdir.name}/my_source.csv\"\n",
    "pd.DataFrame(samples).to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0b27a",
   "metadata": {},
   "source": [
    "Now, we can read the source using the CsvDriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from squirrel.driver import CsvDriver\n",
    "\n",
    "driver = CsvDriver(csv_path)\n",
    "df = pd.DataFrame(driver.get_iter().collect()).set_index(\"Index\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913c5ed",
   "metadata": {},
   "source": [
    "We will convert the data into an RDD first.\n",
    "For this we need to provide a schema.\n",
    "Note that the schema is the only data-source-specific part in the pipeline.\n",
    "As long as we provide the correct schema, we can use the pipeline with any driver or store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"index\", IntegerType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"identifier\", StringType(), False),\n",
    "        StructField(\"age\", IntegerType(), False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "parallel_collection_rdd = spark.sparkContext.parallelize(driver.get_iter())\n",
    "df = spark.createDataFrame(parallel_collection_rdd, SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864f9aa",
   "metadata": {},
   "source": [
    "RDD is ready.\n",
    "Now we can write the data into shards.\n",
    "We opt for the SquirrelStore that is used by the MessagepackDriver here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from squirrel.serialization import MessagepackSerializer\n",
    "from squirrel.store import SquirrelStore\n",
    "\n",
    "\n",
    "def save_iterable_as_shard(it, url) -> None:\n",
    "    \"\"\"Helper to save a shard into a messagepack store using squirrel.\"\"\"\n",
    "    SquirrelStore(url, serializer=MessagepackSerializer()).set(value=list(it))\n",
    "\n",
    "\n",
    "tmpdir2 = tempfile.TemporaryDirectory()\n",
    "N_SHARDS = 10\n",
    "\n",
    "_ = (\n",
    "    df.rdd.map(lambda row: row.asDict())\n",
    "    .coalesce(N_SHARDS)\n",
    "    .foreachPartition(partial(save_iterable_as_shard, url=tmpdir2.name))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150adb08",
   "metadata": {},
   "source": [
    "Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir.cleanup()\n",
    "tmpdir2.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
