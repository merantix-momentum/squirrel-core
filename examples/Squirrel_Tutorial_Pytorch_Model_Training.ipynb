{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a943e60",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/merantix/mxlabs-datasets/blob/main/examples/Squirrel_Tutorial_Pytorch_Model_Training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e59921",
   "metadata": {},
   "source": [
    "# Install Squirrel and Squirrel Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf5492",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install keyring keyrings.google-artifactregistry-auth\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b5122",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install squirrel-core squirrel-datasets --extra-index=https://europe-west1-python.pkg.dev/mx-labs-devops/labs-pypi-registry/simple/ --ignore-requires-python --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe500fe",
   "metadata": {},
   "source": [
    "# MNIST dataset construction using Squirrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "import PIL\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as tud\n",
    "import torchvision.transforms as tr\n",
    "\n",
    "from squirrel.catalog import Catalog\n",
    "from squirrel.iterstream.torch_composables import TorchIterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67476cf6",
   "metadata": {},
   "source": [
    "This tutorial gives a brief overview of how to load a standard dataset in Squirrel to train a simple Neural Network.\n",
    "We start the introduction using the seminal MNIST dataset.\n",
    "\n",
    "The basic entrypoint for loading datasets in Squirrel is the `Catalog` API.\n",
    "We can instantiate a catalog straightforwardly and obtain a list of all registered datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3523c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = Catalog.from_plugins()\n",
    "print(sorted(ca.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2be0fa",
   "metadata": {},
   "source": [
    "To load the MNIST data we simply use a standard dictionary API and call `.get_driver()` to obtain the `Driver`.\n",
    "In this case the resulting driver is a lightweight Huggingface wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47831d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = ca[\"mnist\"].get_driver()\n",
    "type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02535999",
   "metadata": {},
   "source": [
    "Now let's get going to actually load some data and look at a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd079a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.get_iter(\"train\").take(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2b5bf",
   "metadata": {},
   "source": [
    "The dataloader is returning a list of dictionaries.\n",
    "Each of them contains a `PIL.Image.Image` accessible via the _image_-key and a integer via the _label_-key.\n",
    "So let's look at the data.\n",
    "For this we first create a grid plotting function for PIL images and then use the MNIST driver to return images.\n",
    "Note that we use the `map` functionality to select only the _image_ key from the data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc73fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(imgs: t.List[PIL.Image.Image], nrows: int, ncols: int) -> PIL.Image.Image:\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = PIL.Image.new(\"RGB\", size=(ncols * w, nrows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % ncols * w, i // ncols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "imgs = mnist.get_iter(\"train\").take(25).map(lambda x: x[\"image\"]).collect()\n",
    "grid(imgs, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29107b",
   "metadata": {},
   "source": [
    "Looks indeed like MNIST!\n",
    "\n",
    "Let's move on to create a training dataloader.\n",
    "Using the `map` function of the driver, we can easily incorporate the necessary data augmentations.\n",
    "We rely on `torchvision` transforms to construct the augmentation pipeline.\n",
    "We first define the train and test transforms, which in this case only differ by adding some regularizing noise to the training data.\n",
    "Otherwise it consists of casting the PIL image to a `torch.Tensor` object and centering the data.\n",
    "The `augmentation` method will in the next step be used as the `lambda` function object for the driver's `map` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augment = tr.Compose([tr.ToTensor(), tr.Lambda(lambda x: (255 * x + torch.rand_like(x)) / 256 - 0.5)])\n",
    "\n",
    "\n",
    "test_augment = tr.Compose([tr.ToTensor(), tr.Lambda(lambda x: 255 * x / 256 - 0.5)])\n",
    "\n",
    "\n",
    "def augmentation(image: PIL.Image, augmentation: tr.Compose) -> torch.Tensor:\n",
    "    return augmentation(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177ba97",
   "metadata": {},
   "source": [
    "Now that we constructed the augmentation let's put the pieces together.\n",
    "We create the train and test data drivers as already demonstrated previously, then map the data-augmentation over the individual data samples.\n",
    "For the sake of simplicity and speed of training we select a subset of the training and the test data.\n",
    "Finally we compose the driver with a `TorchIterable` in order to make it a torch dataset.\n",
    "This is necessary in order to use the standard `torch.utils.data.DataLoader` API.\n",
    "With this the dataloading with squirrel becomes a drop-in replacement for any previous training loops relying on the torch dataloading mechanism.\n",
    "\n",
    "One detail to be aware of: When applying the data augmentation, note that we turn the dictionary into a tuple.\n",
    "This is to be consistent with the standard torch training examples for MNIST, which returns the data in a tuple of `(sample, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = (\n",
    "    ca[\"mnist\"]\n",
    "    .get_driver()\n",
    "    .get_iter(\"train\")\n",
    "    .map(lambda r: (augmentation(r[\"image\"], train_augment), r[\"label\"]))\n",
    "    .take(4000)\n",
    "    .compose(TorchIterable)\n",
    ")\n",
    "\n",
    "mnist_test = (\n",
    "    ca[\"mnist\"]\n",
    "    .get_driver()\n",
    "    .get_iter(\"train\")\n",
    "    .map(lambda r: (augmentation(r[\"image\"], test_augment), r[\"label\"]))\n",
    "    .take(200)\n",
    "    .compose(TorchIterable)\n",
    ")\n",
    "\n",
    "train_loader = tud.DataLoader(mnist_train, batch_size=20)\n",
    "test_loader = tud.DataLoader(mnist_test, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c435ca",
   "metadata": {},
   "source": [
    "The remaining parts are fairly standard and for more details we refer the reader to PyTorch tutorials explaining how to set up the training.\n",
    "In summary: we first define an evaluation function that measures the accuracy on a given test dataset, then define a standard MLP network with GeLU activation and BatchNorm layers.\n",
    "Finally we define the loss-function for multi-class classification and define our optimizer (in this case SGD with momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net: nn.Module, loader: tud.DataLoader) -> t.Dict[str, float]:\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        accs = []\n",
    "        for b, lbl in tqdm(loader, desc=\"eval\", leave=False):\n",
    "            pred = net(b.reshape(-1, 28 ** 2))\n",
    "            accs += (pred.argmax(-1) == lbl.flatten()).numpy().tolist()\n",
    "\n",
    "    return float(np.mean(accs))\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(28 ** 2, 1024),\n",
    "    nn.GELU(),\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.GELU(),\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(1024, 10),\n",
    ")\n",
    "\n",
    "xent = nn.CrossEntropyLoss()\n",
    "opter = optim.SGD(params=net.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4aa459",
   "metadata": {},
   "source": [
    "Now let's train the model.\n",
    "As you can see from the code it is a standard layout of a torch training loop and nothing refers to specifics of squirrel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d93237",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (b, lbl) in tqdm(enumerate(train_loader)):\n",
    "    if idx % 20 == 0:\n",
    "        print(f\"step: {idx:03d}, accuracy: {evaluate(net, test_loader)}\")\n",
    "    net.train()\n",
    "    opter.zero_grad()\n",
    "    pred = net(b.reshape(-1, 28 ** 2))\n",
    "    loss = xent(pred, lbl)\n",
    "    loss.backward()\n",
    "    opter.step()\n",
    "\n",
    "print(f\"step: {idx:03d}, accuracy: {evaluate(net, test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2cb04",
   "metadata": {},
   "source": [
    "This concludes the _Hello World!_ example of training a deep neural network with squirrel dataloaders. Play and have fun! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
