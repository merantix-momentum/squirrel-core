{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2d7553",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/merantix/mxlabs-datasets/blob/main/examples/Squirrel_Tutorial_Create_Squirrel_Store.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50498336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keyring keyrings.google-artifactregistry-auth\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install squirrel-core squirrel-datasets pyspark --extra-index=https://europe-west1-python.pkg.dev/mx-labs-devops/labs-pypi-registry/simple/ --ignore-requires-python --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f63561",
   "metadata": {},
   "source": [
    "# Squirrel Store\n",
    "\n",
    "`SquirrelStore` is responsible for reading and writing the data. Inheriting from `squirrel.store.AbstractStore`, it defines three main methods, `set`, `get`, and `keys`. `SquirrelStore` requires a serializer. Two serializers are provided, namely `MessagepackSerializer` and `JsonSerializer`, and it's straightforward to write your own. A store can be instantiated as the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca610212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "from squirrel.store import SquirrelStore\n",
    "from squirrel.serialization import MessagepackSerializer\n",
    "from squirrel.driver import MessagepackDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf00d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "msg_store = SquirrelStore(url=tmpdir.name, serializer=MessagepackSerializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d344f",
   "metadata": {},
   "source": [
    "You can get an instance of a store from driver too. This is a recommended approach, unless low-level control is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = MessagepackDriver(tmpdir.name)\n",
    "store = driver.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4fc242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert isinstance(store, SquirrelStore)\n",
    "# assert len(list(store.keys())) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101f97d",
   "metadata": {},
   "source": [
    "# Creating a SquirrelStore\n",
    "\n",
    "\n",
    "## First approach: SquirrelStore itself\n",
    "You can use the low-level map interface of the store to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d751bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(i):\n",
    "    return {\"image\": np.random.random((3, 3, 3)), \"label\": np.random.choice([1, 2]), \"metadata\": {\"key\": i}}\n",
    "\n",
    "\n",
    "samples = [get_sample(i) for i in range(100)]\n",
    "shards = [samples[i : i + 10] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shard in shards:\n",
    "    store.set(shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78124e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(store.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81cb8d",
   "metadata": {},
   "source": [
    "`set()` method accepts an optional argument `key`. If not provided, a random name is automatically assigned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6573a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in store.keys():\n",
    "    shard = store.get(key)\n",
    "    for sample in shard:\n",
    "        print(sample)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c092d",
   "metadata": {},
   "source": [
    "## Second approach: Iterstream api\n",
    "\n",
    "`SquirrelStore` does not buffer any data, as soon as `set()` is called, the data is written to the store. Because of this, writing to the store can be easily paralellized. In the following example, we use `async_map` from `Iterstream` module to write shards to the store in parallel, and read from the store in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbcc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from squirrel.iterstream import IterableSource\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "store = MessagepackDriver(tmpdir.name).store\n",
    "\n",
    "IterableSource(shards).async_map(store.set).join()\n",
    "# assert len(list(store.keys())) == 10\n",
    "\n",
    "samples = IterableSource(store.keys()).async_map(store.get).flatten().collect()\n",
    "# assert len(samples) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51caaa",
   "metadata": {},
   "source": [
    "## Reading and writing to the store using Dask\n",
    "\n",
    "Scaling out using dask is as easy as replacing `async_map(store.set)` with `async_map(store.set, executor=dask.distributed.Client())` in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca45c2c",
   "metadata": {},
   "source": [
    "## Reading and writing to the store using Spark\n",
    "\n",
    "Squirrel makes it a breeze to scale out any data workload. To illustrate this using Spark, we first create a squirrel store and write some data to it, then read from this store into a spark dataframe, then write back from the dataframe into another store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48467b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "driver = MessagepackDriver(tmpdir.name)\n",
    "store = driver.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04070b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(i):\n",
    "    return {\n",
    "        \"name\": np.random.choice([\"John\", \"Jane\"]),\n",
    "        \"identifier\": int(np.random.choice([1, 2])),\n",
    "        \"age\": int(np.random.choice([20, 30])),\n",
    "    }\n",
    "\n",
    "\n",
    "samples = [get_sample(i) for i in range(100)]\n",
    "shards = [samples[i : i + 10] for i in range(10)]\n",
    "\n",
    "IterableSource(shards).async_map(store.set).join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aed958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(list(store.keys())) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c022849",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"identifier\", StringType(), False),\n",
    "        StructField(\"age\", IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parallel_collection_rdd = spark.sparkContext.parallelize(driver.get_iter())\n",
    "df = spark.createDataFrame(parallel_collection_rdd, SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir.cleanup()\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "driver = MessagepackDriver(tmpdir.name)\n",
    "store = driver.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def save_iterable_as_shard(_it, _url) -> None:\n",
    "    \"\"\"Helper to save a shard into a messagepack store using squirrel\"\"\"\n",
    "    SquirrelStore(_url, serializer=MessagepackSerializer()).set(value=[i for i in _it])\n",
    "\n",
    "\n",
    "num_shards = 10\n",
    "_ = (\n",
    "    df.rdd.map(lambda row: row.asDict())\n",
    "    .coalesce(num_shards)\n",
    "    .foreachPartition(partial(save_iterable_as_shard, _url=tmpdir.name))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150adb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(list(store.keys())) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5fcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
